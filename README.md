# ğŸš€ RAG Ollama Plugin

Plugin Claude Code **gÃ©nÃ©rique et adaptatif** pour crÃ©er N'IMPORTE QUELLE application RAG (Retrieval-Augmented Generation) avec FastAPI, LangChain, Qdrant, Ollama et Next.js.

## âœ¨ CaractÃ©ristiques

- **ğŸ§  Profiling Intelligent**: Pose des questions pour comprendre votre projet et gÃ©nÃ¨re l'architecture optimale
- **âš¡ OptimisÃ© M1 Pro 16GB**: Configuration prÃ©-optimisÃ©e pour Apple Silicon
- **ğŸ”§ Adaptatif**: S'adapte Ã  tout type de projet RAG (chatbot, recommandation, Q&A, etc.)
- **ğŸ“¦ Monorepo**: Structure moderne avec pnpm workspaces et Turborepo
- **ğŸ³ Docker Ready**: DÃ©ploiement local et production simplifiÃ©

## ğŸ› ï¸ Stack Technique

| Composant | Technologie |
|-----------|-------------|
| Backend | FastAPI (Python 3.10+) |
| Frontend | Next.js 14+ (template ai-chatbot Vercel) |
| RAG | LangChain |
| Vector DB | Qdrant |
| LLM | Ollama (local) |
| Architecture | Monorepo (pnpm + Turborepo) |

## ğŸ“¥ Installation

```bash
# Via le marketplace Claude Code
/plugin install rag-ollama

# Ou manuellement
git clone <repo> ~/.claude/plugins/rag-ollama
```

## ğŸš€ Quick Start

### 1. CrÃ©er un nouveau projet

```bash
/rag-ollama:init-project
```

Le plugin vous posera des questions pour comprendre votre projet:
- Type de projet (chatbot, recommandation, Q&A...)
- Sources de donnÃ©es (API, fichiers, base de donnÃ©es...)
- Features nÃ©cessaires (profiling, multimodal, widgets...)

### 2. Configurer l'environnement

```bash
cd mon-projet-rag
cp .env.example .env
# Ã‰diter .env avec vos configurations
```

### 3. DÃ©marrer les services

```bash
# DÃ©marrer Qdrant
pnpm docker:up

# Installer Ollama et tÃ©lÃ©charger les modÃ¨les
ollama pull mistral:7b-instruct-q4_0
ollama pull nomic-embed-text

# Lancer le dÃ©veloppement
pnpm dev
```

## ğŸ“‹ Commands Disponibles

| Command | Description |
|---------|-------------|
| `/rag-ollama:init-project` | CrÃ©e un nouveau projet via profiling interactif |
| `/rag-ollama:init-monorepo` | Setup monorepo sans profiling |
| `/rag-ollama:add-data-source` | Configure une nouvelle source de donnÃ©es |
| `/rag-ollama:add-feature` | Ajoute une feature optionnelle |
| `/rag-ollama:test-rag` | Teste la pipeline RAG end-to-end |
| `/rag-ollama:benchmark-embeddings` | Compare les modÃ¨les d'embeddings |
| `/rag-ollama:index-incremental` | Indexation incrÃ©mentale des donnÃ©es |
| `/rag-ollama:deploy` | Build et deploy Docker |

## ğŸ¤– Agents Disponibles

| Agent | Description |
|-------|-------------|
| `project-profiler` | Comprend votre projet via questions intelligentes |
| `architecture-designer` | Design l'architecture RAG optimale |
| `data-modeler` | GÃ©nÃ¨re les schemas (Pydantic, TypeScript, Qdrant) |
| `api-integrator` | IntÃ©gration API REST gÃ©nÃ©rique |
| `ollama-optimizer` | Optimisation pour M1 Pro 16GB |
| `qdrant-tuner` | Configuration Qdrant optimale |
| `frontend-customizer` | Personnalisation du template ai-chatbot |
| `feature-generator` | GÃ©nÃ¨re des features custom |

## ğŸ“š Skills (Documentation)

| Skill | Description |
|-------|-------------|
| `chunking-strategies` | StratÃ©gies de dÃ©coupage de texte |
| `retrieval-strategies` | Patterns de recherche RAG |
| `ollama-m1-optimization` | Optimisations Apple Silicon |
| `fastapi-async-patterns` | Best practices FastAPI async |
| `vercel-ai-sdk-rag` | IntÃ©gration Vercel AI SDK |

## ğŸ¯ Exemples de Projets

Le plugin s'adapte Ã  tout type de projet RAG:

### Assistant de cuisine saine
```
Type: Recommandation
Data: API recettes
Features: Profiling nutritionnel, widgets cartes recettes
```

### Documentation interne
```
Type: Q&A
Data: Confluence/Notion API
Features: Citations, recherche avancÃ©e
```

### Coach linguistique
```
Type: Assistant spÃ©cialisÃ©
Data: Contenus pÃ©dagogiques
Features: STT/TTS, feedback temps rÃ©el
```

## âš™ï¸ Architecture Adaptative

Le plugin analyse votre projet et choisit automatiquement:

| CritÃ¨re | Options |
|---------|---------|
| **Chunking** | Recursive (docs) / Semantic (objets) / Code-aware |
| **Retrieval** | Similarity / MMR / Hybrid / Score Threshold |
| **LLM** | llama3.2:3b (rapide) / mistral:7b (Ã©quilibrÃ©) / llama3.1:8b (qualitÃ©) |
| **Embeddings** | nomic-embed-text / mxbai-embed-large / bge-m3 |

## ğŸ’» Optimisations M1 Pro 16GB

Configuration optimisÃ©e pour Apple Silicon:

```bash
# Variables d'environnement recommandÃ©es
export OLLAMA_USE_METAL=1
export OLLAMA_NUM_THREAD=8
export OLLAMA_NUM_GPU=999
export OLLAMA_CONTEXT_SIZE=4096
```

### Combinaisons recommandÃ©es

| Combo | LLM | Embeddings | RAM Total |
|-------|-----|------------|-----------|
| Fast | llama3.2:3b | nomic-embed-text | ~4GB |
| **Balanced** â­ | mistral:7b-q4 | nomic-embed-text | ~6GB |
| Quality | llama3.1:8b-q4 | mxbai-embed-large | ~8GB |

## ğŸ”§ Structure du projet gÃ©nÃ©rÃ©

```
mon-projet-rag/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ backend/           # FastAPI
â”‚   â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”‚   â”œâ”€â”€ api/routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â””â”€â”€ schemas/
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â””â”€â”€ frontend/          # Next.js
â”‚       â”œâ”€â”€ app/
â”‚       â”œâ”€â”€ components/
â”‚       â””â”€â”€ lib/
â”œâ”€â”€ packages/
â”‚   â””â”€â”€ shared/            # Code partagÃ©
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ project-profile.json   # Configuration du projet
â””â”€â”€ .env
```

## ğŸ³ DÃ©ploiement

### Local (Docker Compose)

```bash
/rag-ollama:deploy
# Choisir "Local (dÃ©veloppement)" ou "Local (production)"
```

### Production

```bash
/rag-ollama:deploy
# Choisir "Docker Hub" ou "GitHub Container Registry"
```

## ğŸ” Troubleshooting

### Ollama non accessible

```bash
# VÃ©rifier qu'Ollama est lancÃ©
ollama list

# Si nÃ©cessaire, redÃ©marrer
killall ollama && ollama serve
```

### Qdrant connection error

```bash
# VÃ©rifier que Docker est lancÃ©
docker ps

# RedÃ©marrer Qdrant
pnpm docker:down && pnpm docker:up
```

### Out of memory

1. RÃ©duire `OLLAMA_CONTEXT_SIZE` Ã  2048
2. Utiliser un modÃ¨le plus petit (llama3.2:3b)
3. Fermer les applications gourmandes

## ğŸ¤ Contributing

Contributions bienvenues ! Voir [CONTRIBUTING.md](CONTRIBUTING.md)

## ğŸ“„ License

MIT

---

GÃ©nÃ©rÃ© avec â¤ï¸ par [Claude Code](https://claude.ai/claude-code)
