# Guide d'utilisation du plugin RAG Ollama

Ce document explique comment utiliser le plugin et ce que vous devez fournir au dÃ©marrage d'un projet.

## PrÃ©requis

### 1. Configuration des variables d'environnement

CrÃ©ez un fichier `.env` Ã  la racine de votre projet avec :

```bash
# Ollama (local)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=mistral:7b-instruct-q4_0
OLLAMA_EMBEDDINGS_MODEL=nomic-embed-text

# Qdrant (vector database)
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_COLLECTION=documents

# PostgreSQL (optionnel - pour conversations/metadata)
DATABASE_URL=postgresql://user:password@localhost:5432/rag_db

# GitHub (pour MCP server)
GITHUB_TOKEN=ghp_xxxxxxxxxxxx

# Chemin du projet (pour MCP filesystem)
PROJECT_PATH=/chemin/vers/votre/projet
```

### 2. Services requis

```bash
# Installer et dÃ©marrer Ollama
brew install ollama
ollama serve

# TÃ©lÃ©charger les modÃ¨les recommandÃ©s
ollama pull mistral:7b-instruct-q4_0
ollama pull nomic-embed-text

# DÃ©marrer Qdrant (via Docker)
docker run -d -p 6333:6333 qdrant/qdrant
```

---

## DÃ©marrage d'un nouveau projet

### MÃ©thode 1 : Brief initial (recommandÃ©)

DÃ©crivez simplement votre projet, le plugin analysera et posera uniquement les questions complÃ©mentaires :

```bash
/rag-ollama:init-project

Je veux crÃ©er un assistant de recommandation de recettes saines.
J'ai une API Spoonacular avec ~5000 recettes en JSON.
Les utilisateurs ont des allergies et prÃ©fÃ©rences alimentaires Ã  respecter.
Je voudrais afficher des cartes recettes dans le chat.
```

**RÃ©ponse du plugin :**

```
ğŸ“Š Analyse de ton projet :

âœ… IdentifiÃ© :
   - Type : SystÃ¨me de recommandation
   - Source : API REST (Spoonacular)
   - Format : Objets structurÃ©s (recettes JSON)
   - Volume : ~5000 documents (petit)

ğŸ” InfÃ©rÃ© :
   - Personnalisation : Profiling (allergies/prÃ©fÃ©rences mentionnÃ©es)
   - UI : Cartes riches (cartes recettes demandÃ©es)

â“ Questions complÃ©mentaires :
```

Le plugin posera ensuite uniquement les questions manquantes (nom du projet, frÃ©quence de mise Ã  jour, citations, langue).

### MÃ©thode 2 : Questionnaire guidÃ©

Si vous prÃ©fÃ©rez Ãªtre guidÃ©, lancez simplement la commande sans description :

```bash
/rag-ollama:init-project
```

Le plugin posera alors toutes les questions :
- Type de projet
- Sources de donnÃ©es
- Format des donnÃ©es
- Volume
- Personnalisation
- Widgets UI
- etc.

### Exemples de descriptions initiales

**Assistant documentation :**
```
Un chatbot pour notre documentation technique interne.
200 pages de docs Markdown sur Notion.
Ã‰quipe francophone, besoin de citer les sources.
```

**SystÃ¨me de recommandation produits :**
```
Recommander des produits e-commerce basÃ© sur les prÃ©fÃ©rences.
API catalogue avec 50K produits, mise Ã  jour quotidienne.
Filtrage par catÃ©gorie, prix, disponibilitÃ©.
Afficher des cartes produits avec images.
```

**Q&A support client :**
```
RÃ©pondre aux questions frÃ©quentes des clients.
Base de tickets Zendesk + FAQ existante.
Besoin de prÃ©cision, Ã©viter les mauvaises rÃ©ponses.
```

---

## Structure du projet gÃ©nÃ©rÃ©

AprÃ¨s le profiling, le plugin gÃ©nÃ¨re :

```
mon-projet-rag/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ backend/                    # FastAPI + LangChain
â”‚   â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”‚   â”œâ”€â”€ api/routes/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py         # Endpoint chat RAG
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ rag.py          # Endpoints indexation
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ health.py       # Health checks
â”‚   â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ rag_service.py      # LangChain chains (LCEL)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ollama_service.py   # ChatOllama + OllamaEmbeddings
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ qdrant_service.py   # QdrantVectorStore + retrievers
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ chunking_service.py # Text splitters
â”‚   â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ config.py       # Settings Pydantic
â”‚   â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â””â”€â”€ schemas/
â”‚   â”‚   â””â”€â”€ requirements.txt        # LangChain packages inclus
â”‚   â”‚
â”‚   â””â”€â”€ frontend/                   # Next.js (ai-chatbot Vercel)
â”‚       â”œâ”€â”€ app/(chat)/page.tsx     # Page chat principale
â”‚       â”œâ”€â”€ components/
â”‚       â”‚   â”œâ”€â”€ sources-panel.tsx   # Affichage sources RAG
â”‚       â”‚   â””â”€â”€ ...
â”‚       â””â”€â”€ lib/hooks/
â”‚
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml          # Qdrant + backend + frontend
â”‚
â”œâ”€â”€ project-profile.json            # Configuration du projet
â”œâ”€â”€ .env.example
â”œâ”€â”€ .mcp.json                       # Config MCP servers
â””â”€â”€ pnpm-workspace.yaml
```

---

## Fichier project-profile.json

Ce fichier est gÃ©nÃ©rÃ© automatiquement et contient toute la configuration :

```json
{
  "name": "mon-assistant-recettes",
  "type": "recommendation",
  "description": "Assistant de recommandation de recettes saines",

  "dataSources": [
    {
      "name": "recettes",
      "type": "api",
      "endpoint": "https://api.example.com/recipes",
      "auth": "api_key",
      "refreshInterval": "daily"
    }
  ],

  "architecture": {
    "chunking": "semantic_object",
    "retrieval": "mmr",
    "llm": "mistral:7b-instruct-q4_0",
    "embeddings": "nomic-embed-text"
  },

  "features": {
    "userProfiling": true,
    "multimodal": false,
    "customWidgets": ["recipe-card", "ingredient-list"],
    "voiceInput": false,
    "citations": true
  },

  "constraints": {
    "ramBudget": "8GB",
    "platform": "m1-pro-16gb"
  }
}
```

---

## Commandes disponibles aprÃ¨s init

```bash
# Ajouter une nouvelle source de donnÃ©es
/rag-ollama:add-data-source

# Ajouter une feature optionnelle
/rag-ollama:add-feature

# Tester la pipeline RAG
/rag-ollama:test-rag

# Indexation incrÃ©mentale
/rag-ollama:index-incremental

# DÃ©ployer
/rag-ollama:deploy
```

---

## MCP Servers disponibles

Les serveurs MCP sont configurÃ©s dans `.mcp.json` :

| Serveur | UtilitÃ© pour RAG |
|---------|------------------|
| `docs-langchain` | Documentation LangChain pour RAG |
| `postgres` | AccÃ¨s direct PostgreSQL pour metadata |
| `memory` | MÃ©moire persistante du contexte projet |
| `fetch` | RÃ©cupÃ©rer docs Ollama, Qdrant |
| `sequential-thinking` | Raisonnement complexe architecture |
| `shadcn` | Composants UI pour frontend |

---

## Hooks configurÃ©s

Les hooks dans `.claude/settings.json` s'exÃ©cutent automatiquement :

| Hook | DÃ©clencheur | Action |
|------|-------------|--------|
| `lint-python.sh` | Write/Edit fichier .py | Ruff lint + format |
| `lint-typescript.sh` | Write/Edit fichier .ts/.tsx | ESLint + Prettier |

---

## Exemple complet : Assistant de recettes

### 1. Lancer le profiling

```bash
/rag-ollama:init-project
```

### 2. RÃ©pondre aux questions

```
Type: Recommandation de recettes saines
Source: API REST (Spoonacular ou similar)
Volume: ~5,000 recettes
Features: Profiling nutritionnel, cartes recettes, citations
CritÃ¨res: Respecter allergies, temps de prÃ©paration < 30min
```

### 3. RÃ©sultat

Le plugin gÃ©nÃ¨re automatiquement :
- Backend FastAPI avec **LangChain chains** pour le RAG
- Chunking configurable via `chunking_service.py`
- Retrieval MMR via `qdrant.get_retriever(search_type="mmr")`
- Frontend avec composant RecipeCard
- Filtres Qdrant sur allergies et temps de prÃ©paration

### Services LangChain gÃ©nÃ©rÃ©s

```python
# Exemple d'utilisation des services
from app.services.rag_service import get_rag_service
from app.services.chunking_service import get_chunking_service

# RAG avec streaming
rag = get_rag_service()
async for chunk in rag.query("Recettes vÃ©gÃ©tariennes rapides"):
    print(chunk)

# Ou via chain complÃ¨te
chain = rag.create_rag_chain(search_type="mmr", k=5)
result = chain.invoke({"input": "Recettes sans gluten"})

# Chunking des documents
chunking = get_chunking_service()
docs = chunking.split_text(
    recipe_text,
    strategy="semantic",
    chunk_size=500
)
```

### 4. Tester

```bash
pnpm dev
# Ouvrir http://localhost:3000
# Taper: "SuggÃ¨re-moi des recettes vÃ©gÃ©tariennes rapides"
```

---

## FAQ

### Q: Dois-je fournir un schÃ©ma de donnÃ©es ?

**R:** Non, le plugin analyse un Ã©chantillon de vos donnÃ©es et gÃ©nÃ¨re le schÃ©ma automatiquement (Pydantic, TypeScript, Qdrant payload).

### Q: Puis-je modifier la configuration aprÃ¨s init ?

**R:** Oui, modifiez `project-profile.json` ou utilisez les commandes :
- `/rag-ollama:add-data-source` pour ajouter des sources
- `/rag-ollama:add-feature` pour activer des features

### Q: Comment ajouter des filtres mÃ©tier ?

**R:** DÃ©crivez vos critÃ¨res lors du profiling. L'agent `data-modeler` crÃ©era les indexes Qdrant appropriÃ©s.

### Q: Le frontend est-il personnalisable ?

**R:** Oui, l'agent `frontend-customizer` modifie le template ai-chatbot Vercel existant sans crÃ©er de nouvelles pages.
